{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36900,
     "status": "ok",
     "timestamp": 1637517403100,
     "user": {
      "displayName": "Marco Muscas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMPN4p7OdEpxHE-vMDXuAgIYFhTiNp3yDm66R_hw=s64",
      "userId": "02166065583955404711"
     },
     "user_tz": 0
    },
    "id": "D9uUFaI_K38Y",
    "outputId": "3930d43f-bf7c-4c62-dda4-4fb2eda7c348"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1637517403100,
     "user": {
      "displayName": "Marco Muscas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMPN4p7OdEpxHE-vMDXuAgIYFhTiNp3yDm66R_hw=s64",
      "userId": "02166065583955404711"
     },
     "user_tz": 0
    },
    "id": "gXFtXhC8LCZI",
    "outputId": "8f5ff157-503e-4ced-f886-3ef3c7e4c4b9"
   },
   "outputs": [],
   "source": [
    "#cd \"/content/drive/MyDrive/AML_Assignment_2_2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1637517403600,
     "user": {
      "displayName": "Marco Muscas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMPN4p7OdEpxHE-vMDXuAgIYFhTiNp3yDm66R_hw=s64",
      "userId": "02166065583955404711"
     },
     "user_tz": 0
    },
    "id": "JsxsDrRiLEJo",
    "outputId": "84801347-717a-4bb4-eb07-d4c9612f81a9"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26400,
     "status": "ok",
     "timestamp": 1637517430000,
     "user": {
      "displayName": "Marco Muscas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMPN4p7OdEpxHE-vMDXuAgIYFhTiNp3yDm66R_hw=s64",
      "userId": "02166065583955404711"
     },
     "user_tz": 0
    },
    "id": "NOB_Onu6mxBY",
    "outputId": "4453a80a-4ec3-4d16-95b5-d4f6784b2e9a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Nov 11 17:09:22 2021\n",
    "\n",
    "@author: marco\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ex3_convnet_utils import get_dataset_loaders, weights_init, PrintModelSize, VisualizeFilter\n",
    "from ex3_convnet_utils import ConvNet\n",
    "from ex3_convnet_utils import complete_training_and_validation\n",
    "from ex3_convnet_utils import test_model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gridsearch_data_default_path = \"ex3_data_augmentation_complete.csv\"\n",
    "\n",
    "gridsearch_data = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    gridsearch_data = pd.read_csv(gridsearch_data_default_path, sep = ';', index_col = None)\n",
    "    print(\"Number of lines in csv: \", len(gridsearch_data))\n",
    "    \n",
    "except Exception as E:\n",
    "    print(E)\n",
    "    print(\"No worries! Seems like this is the first run, or maybe the file is actually not there.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "executionInfo": {
     "elapsed": 133000,
     "status": "error",
     "timestamp": 1637517593300,
     "user": {
      "displayName": "Marco Muscas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMPN4p7OdEpxHE-vMDXuAgIYFhTiNp3yDm66R_hw=s64",
      "userId": "02166065583955404711"
     },
     "user_tz": 0
    },
    "id": "-kXNyzztgP3A",
    "outputId": "009e7ed0-fa51-446e-d0a7-d182ac35c1bb"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "index_last_checkpoint = len(gridsearch_data.index) # Will use this to get to the last permutation of hyperparameters\n",
    "    \n",
    "    \n",
    "cols = ['transformations',\n",
    "        'best_model_valid_accuracy',\n",
    "        'early_stopped_valid_accuracy',\n",
    "        'best_model_test_accuracy',\n",
    "        'early_stopped_best_accuracy']\n",
    "\n",
    "\n",
    "data_aug_transforms = []\n",
    "\n",
    "input_size = 3\n",
    "num_classes = 10\n",
    "hidden_size = [128, 512, 512, 512, 512]\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "norm_layer = 'BN'\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 200\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.9\n",
    "reg = 0.005\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "current_iteration = 0\n",
    "\n",
    "\n",
    "\n",
    "all_transformations = [\n",
    "\n",
    "                        [transforms.RandomHorizontalFlip(p=0.3)],\n",
    "                        [transforms.RandomHorizontalFlip(p=0.6)],\n",
    "\n",
    "                        [transforms.RandomRotation(20)],\n",
    "                        [transforms.RandomRotation(60)],\n",
    "                        [transforms.RandomRotation(110)],\n",
    "\n",
    "\n",
    "                        [transforms.RandomHorizontalFlip(p=0.3), transforms.RandomRotation(20)],\n",
    "                        [transforms.RandomHorizontalFlip(p=0.6), transforms.RandomRotation(60)],\n",
    "                        [transforms.RandomHorizontalFlip(p=0.3), transforms.RandomRotation(110)],\n",
    "                        [transforms.RandomHorizontalFlip(p=0.6), transforms.RandomRotation(20)],\n",
    "                        [transforms.RandomHorizontalFlip(p=0.3), transforms.RandomRotation(60)],\n",
    "                        [transforms.RandomHorizontalFlip(p=0.6), transforms.RandomRotation(110)],\n",
    "\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.2, p=1.0)],\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.5, p=1.0)],\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.7, p=1.0)],\n",
    "\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.4, p=1.0),\n",
    "                        transforms.RandomRotation(20)],\n",
    "\n",
    "\n",
    "\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.7, p=1.0),\n",
    "                        transforms.RandomRotation(20)],\n",
    "\n",
    "\n",
    "\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.4, p=1.0),\n",
    "                        transforms.RandomRotation(60)],\n",
    "\n",
    "\n",
    "\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.7, p=1.0),\n",
    "                        transforms.RandomRotation(60)],\n",
    "\n",
    "                        [transforms.RandomPerspective(distortion_scale=0.4, p=1.0),\n",
    "                        transforms.RandomRotation(110)]\n",
    "    \n",
    "                      ]\n",
    "\n",
    "\n",
    "for data_aug_transforms in all_transformations: \n",
    "\n",
    "  if current_iteration >= index_last_checkpoint: # Using the rows of the dataframe, I can go back to the last combination of parameters\n",
    "      \n",
    "\n",
    "\n",
    "      ''' Get the transformed datasets '''\n",
    "      train_loader, val_loader, test_loader = get_dataset_loaders(data_aug_transforms,\n",
    "                                              batch_size,\n",
    "                                              num_training, \n",
    "                                              num_validation)\n",
    "\n",
    "      \n",
    "\n",
    "      ''' Train on the transformed dataset '''\n",
    "      model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer).to(device)\n",
    "      model.apply(weights_init)     \n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "      \n",
    "      \n",
    "\n",
    "      ''' Get the results '''\n",
    "      results = complete_training_and_validation(model = model,\n",
    "                                      num_epochs = num_epochs,\n",
    "                                      train_loader = train_loader,\n",
    "                                      val_loader = val_loader,\n",
    "                                      device = device,\n",
    "                                      learning_rate = learning_rate,\n",
    "                                      learning_rate_decay = learning_rate_decay,\n",
    "                                      reg = reg,\n",
    "                                      batch_size = batch_size)\n",
    "      \n",
    "\n",
    "      ''' Save to csv file '''\n",
    "      \n",
    "      best_model = results[0]\n",
    "      early_stopped_model = results[1]\n",
    "      best_model_accuracy = results[4]\n",
    "      early_stopped_accuracy = results[5]\n",
    "      \n",
    "      best_model_test_accuracy = np.nan\n",
    "      early_stopped_test_accuracy = np.nan\n",
    "      \n",
    "      if best_model != None:\n",
    "          best_model_test_accuracy = test_model(best_model, test_loader, device)\n",
    "          print(\"Best model test accuracy: \", best_model_test_accuracy)\n",
    "          \n",
    "      if early_stopped_model != None:\n",
    "          early_stopped_test_accuracy = test_model(early_stopped_model, test_loader, device)\n",
    "          print(\"Early stopped test accuracy: \", early_stopped_test_accuracy)\n",
    "      \n",
    "\n",
    "      new_row = pd.DataFrame(data = [[str(data_aug_transforms), best_model_accuracy, early_stopped_accuracy, best_model_test_accuracy, early_stopped_test_accuracy]], columns = cols)\n",
    "      \n",
    "      gridsearch_data = gridsearch_data.append(new_row, ignore_index = True)\n",
    "      \n",
    "      gridsearch_data.to_csv(gridsearch_data_default_path, sep = ';', na_rep = 'nan', index = False)                     \n",
    "      print(\"\\n\")\n",
    "  else:\n",
    "      print(\"Skipping iteration, currently at: \", current_iteration)\n",
    "      \n",
    "  current_iteration += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Augmentation_Tests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
